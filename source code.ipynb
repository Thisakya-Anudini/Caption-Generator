{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loads Captions**: It reads the captions.txt file, where each line contains an image ID and a caption. It splits the line into the image ID and caption, and maps them.\n",
    "\n",
    "**Cleans Captions** It converts all the captions to lowercase to ensure uniformity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and cleaning captions...\n",
      "Loaded 8092 image captions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define paths for the dataset\n",
    "CAPTION_PATH = \"D:/Desktop/DL project/Mini project - dataset/Mini project - dataset/captions.txt\"\n",
    "DATASET_PATH = \"D:/Desktop/DL project/Mini project - dataset/Mini project - dataset/Images\"\n",
    "\n",
    "# Function to load captions from the captions.txt file\n",
    "def load_captions(filepath):\n",
    "    captions_dict = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(',', 1)\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            image_id, caption = parts\n",
    "            image_id = image_id.strip().split('.')[0]  # Remove file extension\n",
    "            if image_id not in captions_dict:\n",
    "                captions_dict[image_id] = []\n",
    "            captions_dict[image_id].append(\"startseq \" + caption.strip() + \" endseq\")  # Add startseq and endseq\n",
    "    return captions_dict\n",
    "\n",
    "# Function to clean the captions (convert to lowercase)\n",
    "def clean_captions(captions):\n",
    "    for img_id in captions:\n",
    "        captions[img_id] = [cap.lower() for cap in captions[img_id]]\n",
    "    return captions\n",
    "\n",
    "# Load and clean captions\n",
    "print(\"Loading and cleaning captions...\")\n",
    "captions = load_captions(CAPTION_PATH)\n",
    "captions = clean_captions(captions)\n",
    "print(f\"Loaded {len(captions)} image captions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shuffling Image IDs:** It shuffles the list of image IDs to ensure randomness.\n",
    "\n",
    "**Splits the Dataset:** It divides the shuffled data into three parts:\n",
    "\n",
    "70% for training\n",
    "\n",
    "15% for validation\n",
    "\n",
    "15% for testing\n",
    "\n",
    "Outputs the Split Sizes: After splitting, it prints how many images are in each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train, validation, and test splits...\n",
      "Train: 5664 images, Validation: 1214 images, Test: 1214 images\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to create random splits for train, validation, and test sets\n",
    "def create_splits(captions):\n",
    "    all_img_ids = list(captions.keys())  # Get all image IDs\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    np.random.shuffle(all_img_ids)  # Shuffle image IDs randomly\n",
    "    train_split = int(0.7 * len(all_img_ids))  # 70% for training\n",
    "    val_split = int(0.85 * len(all_img_ids))  # 15% for validation (total 85% for train + validation)\n",
    "    return all_img_ids[:train_split], all_img_ids[train_split:val_split], all_img_ids[val_split:]\n",
    "\n",
    "# Create the splits\n",
    "print(\"Creating train, validation, and test splits...\")\n",
    "train_ids, val_ids, test_ids = create_splits(captions)\n",
    "\n",
    "print(f\"Train: {len(train_ids)} images, Validation: {len(val_ids)} images, Test: {len(test_ids)} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creates Vocabulary:** It flattens all the captions into a single list and then creates a tokenizer, which learns the word-to-integer mapping.\n",
    "\n",
    "**Finds Max Length:** It calculates the maximum length of the captions (in terms of the number of words) to determine the padding length.\n",
    "\n",
    "Outputs Vocabulary Size and Max Length: After creating the tokenizer, it prints the size of the vocabulary and the maximum caption length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocabulary and tokenizer...\n",
      "Vocabulary size: 8497, Max caption length: 40\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Function to create the vocabulary and tokenizer\n",
    "def create_vocabulary(captions):\n",
    "    all_captions = [caption for cap_list in captions.values() for caption in cap_list]  # Flatten all captions into a list\n",
    "    tokenizer = Tokenizer()  # Initialize the tokenizer\n",
    "    tokenizer.fit_on_texts(all_captions)  # Fit tokenizer on the captions\n",
    "    vocab_size = len(tokenizer.word_index) + 1  # +1 for the padding token\n",
    "    max_length = max(len(c.split()) for c in all_captions)  # Maximum length of captions\n",
    "    return tokenizer, vocab_size, max_length\n",
    "\n",
    "# Create vocabulary and tokenizer\n",
    "print(\"Creating vocabulary and tokenizer...\")\n",
    "tokenizer, vocab_size, max_length = create_vocabulary(captions)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}, Max caption length: {max_length}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**InceptionV3 Model:** It loads the InceptionV3 model pre-trained on ImageNet without the top classification layer, as we just need the feature extraction part.\n",
    "\n",
    "**Feature Extraction:** It processes each image, resizes it, converts it to an array, and applies the necessary preprocessing. Then, it uses the model to extract image features.\n",
    "\n",
    "**Saving Features:** The extracted features are saved to a .pkl file to avoid re-processing the images in the future.\n",
    "\n",
    "**Loading Features:** If the features are already saved in the .pkl file, it loads them directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting image features using InceptionV3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8092/8092 [11:31<00:00, 11.69it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:/Desktop/DL project/Mini project - dataset/outputs/custom_image_features.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m features \u001b[38;5;241m=\u001b[39m extract_features(DATASET_PATH, \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(train_ids \u001b[38;5;241m+\u001b[39m val_ids \u001b[38;5;241m+\u001b[39m test_ids)))\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Save features to a pickle file for future use\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeature_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     39\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(features, f)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage features saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/Desktop/DL project/Mini project - dataset/outputs/custom_image_features.pkl'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the path to save features\n",
    "feature_file = \"D:/Desktop/DL project/Mini project - dataset/outputs/custom_image_features.pkl\"\n",
    "\n",
    "# Check if features are already extracted\n",
    "if os.path.exists(feature_file):\n",
    "    print(\"Loading pre-extracted features...\")\n",
    "    with open(feature_file, 'rb') as f:\n",
    "        features = pickle.load(f)\n",
    "else:\n",
    "    print(\"Extracting image features using InceptionV3...\")\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False, pooling='avg')  # Load InceptionV3 without the top layer\n",
    "\n",
    "    # Function to extract features from images\n",
    "    def extract_features(image_dir, image_ids):\n",
    "        features = {}\n",
    "        for img_id in tqdm(image_ids):\n",
    "            file_path = os.path.join(image_dir, img_id + \".jpg\")\n",
    "            if not os.path.exists(file_path):\n",
    "                continue  # Skip if image file doesn't exist\n",
    "            img = load_img(file_path, target_size=(299, 299))  # Load image\n",
    "            img = img_to_array(img)  # Convert image to array\n",
    "            img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "            img = preprocess_input(img)  # Preprocess image for InceptionV3\n",
    "            feature = base_model.predict(img, verbose=0)  # Get image features\n",
    "            features[img_id] = feature.flatten()  # Flatten the feature vector\n",
    "        return features\n",
    "\n",
    "    # Extract features from training, validation, and test images\n",
    "    features = extract_features(DATASET_PATH, list(set(train_ids + val_ids + test_ids)))\n",
    "\n",
    "    # Save features to a pickle file for future use\n",
    "    with open(feature_file, \"wb\") as f:\n",
    "        pickle.dump(features, f)\n",
    "    print(\"Image features saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Directory Creation:** It ensures that the outputs directory is created using os.makedirs(feature_dir, exist_ok=True) before trying to save the feature file.\n",
    "\n",
    "**File Path:** The feature_file path is updated to save inside the outputs folder correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creates Sequences:** It converts the captions into integer sequences using the tokenizer. For each caption, the image feature is paired with the caption sequence, where each word is converted to an integer.\n",
    "\n",
    "**Pads Sequences:** It ensures that the sequences have a uniform length by padding them with zeros where necessary, up to the max_length (the maximum length of captions calculated earlier).\n",
    "\n",
    "**Returns Arrays:** It returns three arrays:\n",
    "\n",
    "**input_images:** The features of the images.\n",
    "\n",
    "**input_seqs:** The padded sequences of words (captions).\n",
    "\n",
    "**output_words:** The next word in the sequence, which will be predicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training sequences...\n",
      "Train samples: 335100\n",
      "Validation samples: 71484\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Function to create sequences from captions and image features\n",
    "def create_sequences(tokenizer, max_length, captions, features, image_ids):\n",
    "    input_images, input_seqs, output_words = [], [], []\n",
    "\n",
    "    for img_id in image_ids:\n",
    "        if img_id not in captions or img_id not in features:\n",
    "            continue\n",
    "        for caption in captions[img_id]:\n",
    "            seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "            for i in range(1, len(seq)):\n",
    "                input_images.append(features[img_id])\n",
    "                input_seqs.append(seq[:i])\n",
    "                output_words.append(seq[i])\n",
    "\n",
    "    # Pad the sequences to ensure uniform length\n",
    "    padded_seqs = pad_sequences(input_seqs, maxlen=max_length, padding='post')\n",
    "    return np.array(input_images), np.array(padded_seqs), np.array(output_words)\n",
    "\n",
    "# Create the sequences for training and validation sets\n",
    "print(\"Creating training sequences...\")\n",
    "train_img, train_seq, train_out = create_sequences(tokenizer, max_length, captions, features, train_ids)\n",
    "val_img, val_seq, val_out = create_sequences(tokenizer, max_length, captions, features, val_ids)\n",
    "\n",
    "print(f\"Train samples: {len(train_img)}\")\n",
    "print(f\"Validation samples: {len(val_img)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Feature Input: The image features are passed through a Dense layer after applying dropout for regularization.\n",
    "\n",
    "Caption Input: The captions are processed through an embedding layer, followed by another dropout layer and an LSTM layer to handle the sequential nature of the captions.\n",
    "\n",
    "Merging the Inputs: The image features and caption features are merged using the add function and passed through a final dense layer.\n",
    "\n",
    "Output Layer: The final output layer is a dense layer with a softmax activation, which predicts the next word in the caption.\n",
    "\n",
    "Model Compilation: The model is compiled with a sparse categorical cross-entropy loss function (since we're predicting words) and the Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the CNN-LSTM model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,175,232</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
       "│                     │                   │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8497</span>)      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,183,729</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │  \u001b[38;5;34m2,175,232\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m524,544\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m525,312\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
       "│                     │                   │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m65,792\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8497\u001b[0m)      │  \u001b[38;5;34m2,183,729\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,474,609</span> (20.88 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,474,609\u001b[0m (20.88 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,474,609</span> (20.88 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,474,609\u001b[0m (20.88 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input, add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Function to build the CNN-LSTM model\n",
    "def build_model(vocab_size, max_length, feature_size):\n",
    "    # Image feature input\n",
    "    inputs1 = Input(shape=(feature_size,))\n",
    "    fe1 = Dropout(0.5)(inputs1)  # Apply dropout for regularization\n",
    "    fe2 = Dense(256, activation='relu')(fe1)  # Dense layer for image features\n",
    "\n",
    "    # Caption input\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)  # Embedding layer for captions\n",
    "    se2 = Dropout(0.5)(se1)  # Dropout layer for regularization\n",
    "    se3 = LSTM(256)(se2)  # LSTM layer for caption generation\n",
    "\n",
    "    # Decoder (merging image features and captions)\n",
    "    decoder1 = add([fe2, se3])  # Merge image features and caption features\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)  # Another Dense layer\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)  # Final softmax layer for output\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "feature_size = train_img.shape[1]  # Image feature size (length of flattened feature vector)\n",
    "print(\"Building the CNN-LSTM model...\")\n",
    "model = build_model(vocab_size, max_length, feature_size)\n",
    "\n",
    "# Summarize the model architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Checkpoint: Saves the model with the lowest validation loss during training.\n",
    "\n",
    "Early Stopping: Stops training if the validation loss does not improve for a set number of epochs (patience = 5), and restores the best weights.\n",
    "\n",
    "Model Training: Trains the model using the training data (train_img, train_seq, and train_out) and validates it on the validation data (val_img, val_seq, and val_out).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the CNN-LSTM model...\n",
      "Epoch 1/20\n",
      "\u001b[1m5236/5236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.2724 - loss: 4.3620\n",
      "Epoch 1: val_loss improved from inf to 3.47563, saving model to D:/Desktop/DL project/Mini project - dataset/outputs/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5236/5236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m429s\u001b[0m 82ms/step - accuracy: 0.2724 - loss: 4.3619 - val_accuracy: 0.3584 - val_loss: 3.4756\n",
      "Epoch 2/20\n",
      "\u001b[1m5236/5236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.3646 - loss: 3.2347\n",
      "Epoch 2: val_loss improved from 3.47563 to 3.36156, saving model to D:/Desktop/DL project/Mini project - dataset/outputs/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5236/5236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m415s\u001b[0m 79ms/step - accuracy: 0.3646 - loss: 3.2347 - val_accuracy: 0.3763 - val_loss: 3.3616\n",
      "Epoch 3/20\n",
      "\u001b[1m5236/5236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.3847 - loss: 2.9645\n",
      "Epoch 3: val_loss improved from 3.36156 to 3.34273, saving model to D:/Desktop/DL project/Mini project - dataset/outputs/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5236/5236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m389s\u001b[0m 74ms/step - accuracy: 0.3847 - loss: 2.9645 - val_accuracy: 0.3853 - val_loss: 3.3427\n",
      "Epoch 4/20\n",
      "\u001b[1m5236/5236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.3958 - loss: 2.8112\n",
      "Epoch 4: val_loss did not improve from 3.34273\n",
      "\u001b[1m5236/5236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 73ms/step - accuracy: 0.3958 - loss: 2.8112 - val_accuracy: 0.3901 - val_loss: 3.3669\n",
      "Epoch 5/20\n",
      "\u001b[1m5236/5236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.4052 - loss: 2.7065\n",
      "Epoch 5: val_loss did not improve from 3.34273\n",
      "\u001b[1m5236/5236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m389s\u001b[0m 74ms/step - accuracy: 0.4052 - loss: 2.7065 - val_accuracy: 0.3905 - val_loss: 3.4017\n",
      "Epoch 6/20\n",
      "\u001b[1m5236/5236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.4128 - loss: 2.6340\n",
      "Epoch 6: val_loss did not improve from 3.34273\n",
      "\u001b[1m5236/5236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m388s\u001b[0m 74ms/step - accuracy: 0.4128 - loss: 2.6340 - val_accuracy: 0.3922 - val_loss: 3.4448\n",
      "Epoch 7/20\n",
      "\u001b[1m5236/5236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.4175 - loss: 2.5744\n",
      "Epoch 7: val_loss did not improve from 3.34273\n",
      "\u001b[1m5236/5236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 74ms/step - accuracy: 0.4175 - loss: 2.5745 - val_accuracy: 0.3936 - val_loss: 3.4744\n",
      "Epoch 8/20\n",
      "\u001b[1m5236/5236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.4227 - loss: 2.5310\n",
      "Epoch 8: val_loss did not improve from 3.34273\n",
      "\u001b[1m5236/5236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m388s\u001b[0m 74ms/step - accuracy: 0.4227 - loss: 2.5310 - val_accuracy: 0.3923 - val_loss: 3.5459\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define the callback for saving the best model during training\n",
    "checkpoint_path = \"D:/Desktop/DL project/Mini project - dataset/outputs/best_model.h5\"\n",
    "checkpoint = ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# Define early stopping to avoid overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the CNN-LSTM model...\")\n",
    "history = model.fit(\n",
    "    [train_img, train_seq], train_out,  # Inputs and outputs for training\n",
    "    validation_data=([val_img, val_seq], val_out),  # Validation data\n",
    "    epochs=20,  # Number of epochs (you can adjust this)\n",
    "    batch_size=64,  # Batch size\n",
    "    callbacks=[early_stopping, checkpoint]  # Callbacks for early stopping and saving the best model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Reloading captions...\n",
      " Loaded 8092 image captions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define path to your captions file\n",
    "CAPTION_PATH = \"D:/Desktop/DL project/Mini project - dataset/Mini project - dataset/captions.txt\"\n",
    "\n",
    "# Reload captions from file\n",
    "def load_captions(filepath):\n",
    "    captions_dict = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(',', 1)\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            image_id, caption = parts\n",
    "            image_id = image_id.strip().split('.')[0]\n",
    "            if image_id not in captions_dict:\n",
    "                captions_dict[image_id] = []\n",
    "            captions_dict[image_id].append(\"startseq \" + caption.strip() + \" endseq\")\n",
    "    return captions_dict\n",
    "\n",
    "# Clean captions (lowercase)\n",
    "def clean_captions(captions):\n",
    "    for img_id in captions:\n",
    "        captions[img_id] = [cap.lower() for cap in captions[img_id]]\n",
    "    return captions\n",
    "\n",
    "# Execute loading\n",
    "print(\" Reloading captions...\")\n",
    "captions = load_captions(CAPTION_PATH)\n",
    "captions = clean_captions(captions)\n",
    "print(f\" Loaded {len(captions)} image captions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Recreating sequences...\n",
      " Train samples: 335100, Validation samples: 71484\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Recreate the image sequences from existing variables\n",
    "def create_sequences(tokenizer, max_length, captions, features, image_ids):\n",
    "    input_images, input_seqs, output_words = [], [], []\n",
    "\n",
    "    for img_id in image_ids:\n",
    "        if img_id not in captions or img_id not in features:\n",
    "            continue\n",
    "        for caption in captions[img_id]:\n",
    "            seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "            for i in range(1, len(seq)):\n",
    "                input_images.append(features[img_id])\n",
    "                input_seqs.append(seq[:i])\n",
    "                output_words.append(seq[i])\n",
    "\n",
    "    padded_seqs = pad_sequences(input_seqs, maxlen=max_length, padding='post')\n",
    "    return np.array(input_images), np.array(padded_seqs), np.array(output_words)\n",
    "\n",
    "# Paths\n",
    "feature_file = \"D:/Desktop/DL project/Mini project - dataset/outputs/custom_image_features.pkl\"\n",
    "\n",
    "# Load pre-extracted image features\n",
    "with open(feature_file, 'rb') as f:\n",
    "    features = pickle.load(f)\n",
    "\n",
    "# Recreate train/val/test splits if not available\n",
    "def create_splits(captions):\n",
    "    all_img_ids = list(captions.keys())\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(all_img_ids)\n",
    "    train_split = int(0.7 * len(all_img_ids))\n",
    "    val_split = int(0.85 * len(all_img_ids))\n",
    "    return all_img_ids[:train_split], all_img_ids[train_split:val_split], all_img_ids[val_split:]\n",
    "\n",
    "train_ids, val_ids, test_ids = create_splits(captions)\n",
    "\n",
    "# Create sequences\n",
    "print(\" Recreating sequences...\")\n",
    "train_img, train_seq, train_out = create_sequences(tokenizer, max_length, captions, features, train_ids)\n",
    "val_img, val_seq, val_out = create_sequences(tokenizer, max_length, captions, features, val_ids)\n",
    "\n",
    "print(f\" Train samples: {len(train_img)}, Validation samples: {len(val_img)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Vocab size: 8497, Max length: 40\n",
      "🚀 Resuming training from Epoch 9...\n",
      "Epoch 9/20\n",
      "\u001b[1m5236/5236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.3915 - loss: 2.9442\n",
      "Epoch 9: val_loss improved from inf to 3.33909, saving model to D:/Desktop/DL project/Mini project - dataset/outputs/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5236/5236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m406s\u001b[0m 77ms/step - accuracy: 0.3915 - loss: 2.9442 - val_accuracy: 0.3863 - val_loss: 3.3391\n",
      "Epoch 10/20\n",
      "\u001b[1m5236/5236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.4016 - loss: 2.8406\n",
      "Epoch 10: val_loss did not improve from 3.33909\n",
      "\u001b[1m5236/5236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m402s\u001b[0m 77ms/step - accuracy: 0.4016 - loss: 2.8406 - val_accuracy: 0.3898 - val_loss: 3.3542\n",
      "Epoch 11/20\n",
      "\u001b[1m5236/5236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.4076 - loss: 2.7486\n",
      "Epoch 11: val_loss did not improve from 3.33909\n",
      "\u001b[1m5236/5236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m407s\u001b[0m 78ms/step - accuracy: 0.4076 - loss: 2.7486 - val_accuracy: 0.3904 - val_loss: 3.3677\n",
      "Epoch 12/20\n",
      "\u001b[1m5236/5236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.4137 - loss: 2.6685\n",
      "Epoch 12: val_loss did not improve from 3.33909\n",
      "\u001b[1m5236/5236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m410s\u001b[0m 78ms/step - accuracy: 0.4137 - loss: 2.6686 - val_accuracy: 0.3926 - val_loss: 3.4175\n",
      "Epoch 13/20\n",
      "\u001b[1m5236/5236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.4182 - loss: 2.6134\n",
      "Epoch 13: val_loss did not improve from 3.33909\n",
      "\u001b[1m5236/5236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m418s\u001b[0m 80ms/step - accuracy: 0.4182 - loss: 2.6134 - val_accuracy: 0.3916 - val_loss: 3.4379\n",
      "Epoch 14/20\n",
      "\u001b[1m5236/5236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.4207 - loss: 2.5784\n",
      "Epoch 14: val_loss did not improve from 3.33909\n",
      "\u001b[1m5236/5236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m427s\u001b[0m 82ms/step - accuracy: 0.4207 - loss: 2.5784 - val_accuracy: 0.3934 - val_loss: 3.5231\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input, add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "# === 1. Recreate tokenizer and calculate vocab_size, max_length ===\n",
    "all_captions = [cap for cap_list in captions.values() for cap in cap_list]\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_length = max(len(cap.split()) for cap in all_captions)\n",
    "feature_size = train_img.shape[1]\n",
    "\n",
    "print(f\" Vocab size: {vocab_size}, Max length: {max_length}\")\n",
    "\n",
    "# === 2. Rebuild the exact model architecture ===\n",
    "def build_model(vocab_size, max_length, feature_size):\n",
    "    inputs1 = Input(shape=(feature_size,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_model(vocab_size, max_length, feature_size)\n",
    "\n",
    "# === 3. Load weights from epoch 8 ===\n",
    "model.load_weights(\"D:/Desktop/DL project/Mini project - dataset/outputs/best_model.h5\")\n",
    "\n",
    "# === 4. Callbacks ===\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"D:/Desktop/DL project/Mini project - dataset/outputs/best_model.h5\",\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# === 5. Continue training from epoch 9 ===\n",
    "print(\"🚀 Resuming training from Epoch 9...\")\n",
    "\n",
    "history = model.fit(\n",
    "    [train_img, train_seq], train_out,\n",
    "    validation_data=([val_img, val_seq], val_out),\n",
    "    epochs=20,\n",
    "    initial_epoch=8,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping, checkpoint]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20\n",
      "\n",
      "Epoch 15: val_loss did not improve from 3.33909\n",
      "5236/5236 - 422s - 81ms/step - accuracy: 0.4006 - loss: 2.8361 - val_accuracy: 0.3877 - val_loss: 3.3780\n",
      "Epoch 16/20\n",
      "\n",
      "Epoch 16: val_loss did not improve from 3.33909\n",
      "5236/5236 - 409s - 78ms/step - accuracy: 0.4073 - loss: 2.7463 - val_accuracy: 0.3915 - val_loss: 3.3930\n",
      "Epoch 17/20\n",
      "\n",
      "Epoch 17: val_loss did not improve from 3.33909\n",
      "5236/5236 - 407s - 78ms/step - accuracy: 0.4121 - loss: 2.6811 - val_accuracy: 0.3940 - val_loss: 3.4353\n",
      "Epoch 18/20\n",
      "\n",
      "Epoch 18: val_loss did not improve from 3.33909\n",
      "5236/5236 - 405s - 77ms/step - accuracy: 0.4162 - loss: 2.6285 - val_accuracy: 0.3939 - val_loss: 3.4685\n",
      "Epoch 19/20\n",
      "\n",
      "Epoch 19: val_loss did not improve from 3.33909\n",
      "5236/5236 - 408s - 78ms/step - accuracy: 0.4202 - loss: 2.5875 - val_accuracy: 0.3929 - val_loss: 3.5271\n",
      "Epoch 20/20\n",
      "\n",
      "Epoch 20: val_loss did not improve from 3.33909\n",
      "5236/5236 - 406s - 78ms/step - accuracy: 0.4232 - loss: 2.5516 - val_accuracy: 0.3927 - val_loss: 3.5171\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [train_img, train_seq], train_out,\n",
    "    validation_data=([val_img, val_seq], val_out),\n",
    "    epochs=20,  # Total number of epochs to train (this continues from Epoch 14)\n",
    "    initial_epoch=14,  # Starts training from Epoch 14 (Epochs are 0-indexed)\n",
    "    batch_size=64,\n",
    "    verbose=2,  # Show the full logs for each epoch\n",
    "    callbacks=[early_stopping, checkpoint]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the Model on Test Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2232/2232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 16ms/step - accuracy: 0.3885 - loss: 3.3939\n",
      "Test Loss: 3.4419095516204834, Test Accuracy: 0.384755402803421\n"
     ]
    }
   ],
   "source": [
    "test_img, test_seq, test_out = create_sequences(tokenizer, max_length, captions, features, test_ids)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate([test_img, test_seq], test_out, verbose=1)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Captions for Test Images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(model, image_path, tokenizer, max_length, feature_file=None):\n",
    "    # Load and preprocess image\n",
    "    img = load_img(image_path, target_size=(299, 299))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)  # Preprocess for InceptionV3\n",
    "\n",
    "    # Extract features from the image\n",
    "    if feature_file:\n",
    "        with open(feature_file, \"rb\") as f:\n",
    "            features = pickle.load(f)\n",
    "        img_id = os.path.basename(image_path).split('.')[0]\n",
    "        feature = features.get(img_id)  # Get image features for the specific image\n",
    "    else:\n",
    "        feature = base_model.predict(img, verbose=0).flatten()  # Flatten to match model input\n",
    "\n",
    "    # Initialize the caption sequence with 'startseq'\n",
    "    in_text = 'startseq'\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Convert caption to sequence and pad\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length, padding='post')\n",
    "\n",
    "        # Predict the next word\n",
    "        yhat = model.predict([np.array([feature]), sequence], verbose=0)  # Ensure image feature is a batch of 1\n",
    "        yhat = np.argmax(yhat)\n",
    "\n",
    "        # Map predicted word index to word\n",
    "        word = ''\n",
    "        for w, idx in tokenizer.word_index.items():\n",
    "            if idx == yhat:\n",
    "                word = w\n",
    "                break\n",
    "\n",
    "        # Stop if the word is 'endseq'\n",
    "        if word is None or word == 'endseq':\n",
    "            break\n",
    "\n",
    "        # Append predicted word to the input caption\n",
    "        in_text += ' ' + word\n",
    "\n",
    "    # Clean up the caption\n",
    "    caption = in_text.replace('startseq', '').replace('endseq', '').strip()\n",
    "    return caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-extracted features...\n",
      "Extracting features for new image: D:/Downloads/test3.jpg\n",
      "Features saved.\n",
      "Generating caption for image: D:/Desktop/DL project/Mini project - dataset/Mini project - dataset/Images/335588286_f67ed8c9f9.jpg\n",
      "Generated Caption: a dog is running through a field of water\n",
      "--------------------------------------------------\n",
      "Generating caption for image: D:/Desktop/DL project/Mini project - dataset/Mini project - dataset/Images/507758961_e63ca126cc.jpg\n",
      "Generated Caption: a young boy in a red shirt is jumping over a trampoline\n",
      "--------------------------------------------------\n",
      "Generating caption for image: D:/Desktop/DL project/Mini project - dataset/Mini project - dataset/Images/2021602343_03023e1fd1.jpg\n",
      "Generated Caption: a basketball player in a white shirt is holding a basketball\n",
      "--------------------------------------------------\n",
      "Generating caption for image: D:/Desktop/DL project/Mini project - dataset/Mini project - dataset/Images/2255685792_f70474c6db.jpg\n",
      "Generated Caption: a man in a red shirt is standing on a rock\n",
      "--------------------------------------------------\n",
      "Generating caption for image: D:/Desktop/DL project/Mini project - dataset/Mini project - dataset/Images/2439031566_2e0c0d3550.jpg\n",
      "Generated Caption: a man is riding a bicycle on a track\n",
      "--------------------------------------------------\n",
      "Generating caption for image: D:/Downloads/test.jpg\n",
      "Generated Caption: a man in a blue shirt is standing on the beach\n",
      "--------------------------------------------------\n",
      "Generating caption for image: D:/Downloads/test1.jpg\n",
      "Generated Caption: a man in a black shirt is playing with a dog\n",
      "--------------------------------------------------\n",
      "Generating caption for image: D:/Downloads/test3.jpg\n",
      "Generated Caption: a dog runs through a field of grass\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import numpy as np\n",
    "\n",
    "# Function to extract image features using InceptionV3\n",
    "def extract_image_features(image_paths, feature_file=None):\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features = {}\n",
    "\n",
    "    if feature_file and os.path.exists(feature_file):\n",
    "        print(\"Loading pre-extracted features...\")\n",
    "        with open(feature_file, 'rb') as f:\n",
    "            features = pickle.load(f)\n",
    "    \n",
    "    for image_path in image_paths:\n",
    "        img_id = os.path.basename(image_path).split('.')[0]\n",
    "        if img_id not in features:  # If feature is not already extracted\n",
    "            print(f\"Extracting features for new image: {image_path}\")\n",
    "            img = load_img(image_path, target_size=(299, 299))  # Resize to InceptionV3 input size\n",
    "            img = img_to_array(img)  # Convert image to numpy array\n",
    "            img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "            img = preprocess_input(img)  # Preprocess image for InceptionV3\n",
    "\n",
    "            feature = base_model.predict(img, verbose=0)  # Get image features from InceptionV3\n",
    "            features[img_id] = feature.flatten()  # Flatten the feature vector\n",
    "\n",
    "    # Save the features to the file for future use\n",
    "    if feature_file:\n",
    "        with open(feature_file, 'wb') as f:\n",
    "            pickle.dump(features, f)\n",
    "        print(\"Features saved.\")\n",
    "\n",
    "    return features\n",
    "\n",
    "# Function to generate caption for a given image\n",
    "def generate_caption(model, image_path, tokenizer, max_length, features, feature_file=None):\n",
    "    img_id = os.path.basename(image_path).split('.')[0]\n",
    "    feature = features.get(img_id)\n",
    "\n",
    "    if feature is None:\n",
    "        print(f\"Feature not found for {image_path}. Extracting features...\")\n",
    "        features = extract_image_features([image_path], feature_file)\n",
    "        feature = features.get(img_id)\n",
    "\n",
    "    # Initialize caption sequence with 'startseq'\n",
    "    in_text = 'startseq'\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]  # Convert to sequence\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length, padding='post')  # Pad sequence\n",
    "\n",
    "        # Predict the next word in the caption\n",
    "        yhat = model.predict([np.array([feature]), sequence], verbose=0)  # Model prediction\n",
    "        yhat = np.argmax(yhat)  # Get the index of the highest probability\n",
    "\n",
    "        # Map the predicted word index to the word\n",
    "        word = ''\n",
    "        for w, idx in tokenizer.word_index.items():\n",
    "            if idx == yhat:\n",
    "                word = w\n",
    "                break\n",
    "\n",
    "        # Break if 'endseq' is predicted\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "\n",
    "        # Append predicted word to input sequence\n",
    "        in_text += ' ' + word\n",
    "\n",
    "    caption = in_text.replace('startseq', '').replace('endseq', '').strip()\n",
    "    return caption\n",
    "\n",
    "# Function to generate captions for a list of images\n",
    "def generate_captions_for_images(model, image_paths, tokenizer, max_length, feature_file=None):\n",
    "    features = extract_image_features(image_paths, feature_file)  # Extract features for all images\n",
    "    for image_path in image_paths:\n",
    "        print(f\"Generating caption for image: {image_path}\")\n",
    "        try:\n",
    "            caption = generate_caption(model, image_path, tokenizer, max_length, features, feature_file)\n",
    "            print(f\"Generated Caption: {caption}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating caption for {image_path}: {e}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# List of images from the dataset and new test images\n",
    "\n",
    "image_paths = [\n",
    "    \"D:/Desktop/DL project/Mini project - dataset/Mini project - dataset/Images/335588286_f67ed8c9f9.jpg\",\n",
    "    \"D:/Desktop/DL project/Mini project - dataset/Mini project - dataset/Images/507758961_e63ca126cc.jpg\",\n",
    "    \"D:/Desktop/DL project/Mini project - dataset/Mini project - dataset/Images/2021602343_03023e1fd1.jpg\",\n",
    "    \"D:/Desktop/DL project/Mini project - dataset/Mini project - dataset/Images/2255685792_f70474c6db.jpg\",\n",
    "    \"D:/Desktop/DL project/Mini project - dataset/Mini project - dataset/Images/2439031566_2e0c0d3550.jpg\",\n",
    "    \"D:/Downloads/test.jpg\",  # Existing test image\n",
    "    \"D:/Downloads/test1.jpg\",  # Existing test image\n",
    "    \"D:/Downloads/test3.jpg\"  # New test image\n",
    "]\n",
    "\n",
    "# Generate captions for all images including test3.jpg\n",
    "generate_captions_for_images(model, image_paths, tokenizer, max_length, feature_file=\"D:/Desktop/DL project/Mini project - dataset/outputs/custom_image_features.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter Finalization and Fine-tuning:**\n",
    "\n",
    "Hyperparameters:\n",
    "\n",
    "Batch size is set to 32.\n",
    "\n",
    "Learning rate is 0.001 (you can adjust based on training behavior).\n",
    "\n",
    "Dropout rate is set to 0.3 for regularization.\n",
    "\n",
    "LSTM units are set to 128.\n",
    "\n",
    "Callbacks:\n",
    "\n",
    "ModelCheckpoint: Saves the model weights whenever there is improvement in the validation loss.\n",
    "\n",
    "EarlyStopping: Stops the training if validation loss does not improve for 5 epochs, and restores the best weights.\n",
    "\n",
    "Model Compilation:\n",
    "\n",
    "Using the Adam optimizer with the defined learning rate and sparse categorical cross-entropy loss function suitable for multi-class classification.\n",
    "\n",
    "Model Saving:\n",
    "\n",
    "After fine-tuning, the model is saved as final_finetuned_model.h5 in the given output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer and max_length saved to D:/Desktop/DL project/Mini project - dataset/outputs/tokenizer.pkl and D:/Desktop/DL project/Mini project - dataset/outputs/max_length.pkl\n"
     ]
    }
   ],
   "source": [
    "# Create the tokenizer from scratch using the captions dataset\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Recreate the tokenizer using all captions\n",
    "all_captions = [caption for cap_list in captions.values() for caption in cap_list]\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "\n",
    "# Save the tokenizer as a pickle file\n",
    "tokenizer_path = \"D:/Desktop/DL project/Mini project - dataset/outputs/tokenizer.pkl\"\n",
    "with open(tokenizer_path, 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "# Save max_length (which you can also load again)\n",
    "max_length = max(len(c.split()) for c in all_captions)\n",
    "maxlen_path = \"D:/Desktop/DL project/Mini project - dataset/outputs/max_length.pkl\"\n",
    "with open(maxlen_path, 'wb') as f:\n",
    "    pickle.dump(max_length, f)\n",
    "\n",
    "print(f\"Tokenizer and max_length saved to {tokenizer_path} and {maxlen_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded with 8496 words.\n",
      "Max length for captions: 40\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load tokenizer and max_length\n",
    "tokenizer_path = \"D:/Desktop/DL project/Mini project - dataset/outputs/tokenizer.pkl\"\n",
    "maxlen_path = \"D:/Desktop/DL project/Mini project - dataset/outputs/max_length.pkl\"\n",
    "\n",
    "# Load tokenizer\n",
    "with open(tokenizer_path, \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# Load max_length\n",
    "with open(maxlen_path, \"rb\") as f:\n",
    "    max_length = pickle.load(f)\n",
    "\n",
    "# Verify the tokenizer and max_length loaded correctly\n",
    "print(f\"Tokenizer loaded with {len(tokenizer.word_index)} words.\")\n",
    "print(f\"Max length for captions: {max_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validation data saved successfully to .npy files!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Save training data (train_img, train_seq, train_out) to .npy files\n",
    "np.save(\"D:/Desktop/DL project/Mini project - dataset/outputs/train_img_data.npy\", train_img)\n",
    "np.save(\"D:/Desktop/DL project/Mini project - dataset/outputs/train_seq_data.npy\", train_seq)\n",
    "np.save(\"D:/Desktop/DL project/Mini project - dataset/outputs/train_out_data.npy\", train_out)\n",
    "\n",
    "# Save validation data (val_img, val_seq, val_out) to .npy files\n",
    "np.save(\"D:/Desktop/DL project/Mini project - dataset/outputs/val_img_data.npy\", val_img)\n",
    "np.save(\"D:/Desktop/DL project/Mini project - dataset/outputs/val_seq_data.npy\", val_seq)\n",
    "np.save(\"D:/Desktop/DL project/Mini project - dataset/outputs/val_out_data.npy\", val_out)\n",
    "\n",
    "print(\"Training and validation data saved successfully to .npy files!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validation data saved successfully to .npy files!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming train_img, train_seq, train_out, val_img, val_seq, val_out are already created and available\n",
    "\n",
    "# Save training data (train_img, train_seq, train_out) to .npy files\n",
    "np.save(\"D:/Desktop/DL project/Mini project - dataset/outputs/train_img_data.npy\", train_img)\n",
    "np.save(\"D:/Desktop/DL project/Mini project - dataset/outputs/train_seq_data.npy\", train_seq)\n",
    "np.save(\"D:/Desktop/DL project/Mini project - dataset/outputs/train_out_data.npy\", train_out)\n",
    "\n",
    "# Save validation data (val_img, val_seq, val_out) to .npy files\n",
    "np.save(\"D:/Desktop/DL project/Mini project - dataset/outputs/val_img_data.npy\", val_img)\n",
    "np.save(\"D:/Desktop/DL project/Mini project - dataset/outputs/val_seq_data.npy\", val_seq)\n",
    "np.save(\"D:/Desktop/DL project/Mini project - dataset/outputs/val_out_data.npy\", val_out)\n",
    "\n",
    "print(\"Training and validation data saved successfully to .npy files!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images shape: (335100, 2048)\n",
      "Training sequences shape: (335100, 40)\n",
      "Training outputs shape: (335100,)\n",
      "Validation images shape: (71484, 2048)\n",
      "Validation sequences shape: (71484, 40)\n",
      "Validation outputs shape: (71484,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the saved .npy files for training and validation data\n",
    "train_img = np.load(\"D:/Desktop/DL project/Mini project - dataset/outputs/train_img_data.npy\")\n",
    "train_seq = np.load(\"D:/Desktop/DL project/Mini project - dataset/outputs/train_seq_data.npy\")\n",
    "train_out = np.load(\"D:/Desktop/DL project/Mini project - dataset/outputs/train_out_data.npy\")\n",
    "val_img = np.load(\"D:/Desktop/DL project/Mini project - dataset/outputs/val_img_data.npy\")\n",
    "val_seq = np.load(\"D:/Desktop/DL project/Mini project - dataset/outputs/val_seq_data.npy\")\n",
    "val_out = np.load(\"D:/Desktop/DL project/Mini project - dataset/outputs/val_out_data.npy\")\n",
    "\n",
    "# Print the shape of the loaded data to ensure everything is correct\n",
    "print(f\"Training images shape: {train_img.shape}\")\n",
    "print(f\"Training sequences shape: {train_seq.shape}\")\n",
    "print(f\"Training outputs shape: {train_out.shape}\")\n",
    "print(f\"Validation images shape: {val_img.shape}\")\n",
    "print(f\"Validation sequences shape: {val_seq.shape}\")\n",
    "print(f\"Validation outputs shape: {val_out.shape}\")\n",
    "\n",
    "# Now you can proceed to train the model with the data you just loaded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning with Keras Tuner:\n",
    "We will use Keras Tuner for hyperparameter tuning. This allows us to search for the best hyperparameters for your model such as learning_rate, dropout_rate, LSTM_units, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input, add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the model-building function with hyperparameters\n",
    "def build_model_with_tuning(hp):\n",
    "    # Image feature input\n",
    "    inputs1 = Input(shape=(train_img.shape[1],))  # Adjust input shape if necessary\n",
    "    fe1 = Dropout(hp.Float('dropout_rate', 0.2, 0.5, step=0.1))(inputs1)  # Tuning dropout rate\n",
    "    fe2 = Dense(256, activation='relu')(fe1)  # Dense layer for image features\n",
    "\n",
    "    # Caption input\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)  # Embedding layer\n",
    "    se2 = Dropout(hp.Float('dropout_rate', 0.2, 0.5, step=0.1))(se1)\n",
    "    se3 = LSTM(hp.Choice('lstm_units', [128, 256, 512]))(se2)  # Tuning LSTM units\n",
    "\n",
    "    # Adjust dimensions to match before merging (using Dense layer)\n",
    "    se3 = Dense(256)(se3)  # Ensure LSTM output matches the size of the image feature layer\n",
    "\n",
    "    # Decoder (Merging image features and captions)\n",
    "    decoder1 = add([fe2, se3])  # Merge image features and caption features\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "    # Compile the model\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer=Adam(learning_rate=hp.Choice('learning_rate', [1e-3, 1e-4, 1e-5])),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the Hyperparameter Tuning:\n",
    "This code will perform hyperparameter tuning by optimizing:\n",
    "\n",
    "dropout_rate (from 0.2 to 0.5),\n",
    "\n",
    "lstm_units (with options 128, 256, 512),\n",
    "\n",
    "learning_rate (with options 1e-3, 1e-4, and 1e-5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [01h 09m 05s]\n",
      "val_loss: 3.836247444152832\n",
      "\n",
      "Best val_loss So Far: 3.836247444152832\n",
      "Total elapsed time: 01h 09m 05s\n",
      "\n",
      "Search: Running Trial #2\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "0.4               |0.2               |dropout_rate\n",
      "128               |256               |lstm_units\n",
      "0.001             |1e-05             |learning_rate\n",
      "\n",
      "Epoch 1/10\n",
      "\u001b[1m5236/5236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 67ms/step - accuracy: 0.2740 - loss: 4.3605 - val_accuracy: 0.3564 - val_loss: 3.5096\n",
      "Epoch 2/10\n",
      "\u001b[1m1169/5236\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:22\u001b[0m 65ms/step - accuracy: 0.3582 - loss: 3.2570"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 14\u001b[0m\n\u001b[0;32m      4\u001b[0m tuner \u001b[38;5;241m=\u001b[39m kt\u001b[38;5;241m.\u001b[39mRandomSearch(\n\u001b[0;32m      5\u001b[0m     build_model_with_tuning,\n\u001b[0;32m      6\u001b[0m     objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_captioning_tuning\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Start the hyperparameter search\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_seq\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m             \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_seq\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_out\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m             \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Number of epochs to run for each trial\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m \u001b[43m             \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Get the best hyperparameters\u001b[39;00m\n\u001b[0;32m     20\u001b[0m best_hp \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mget_best_hyperparameters(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:234\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_end(trial)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:274\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_try_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m    275\u001b[0m         trial\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m trial_module\u001b[38;5;241m.\u001b[39mTrialStatus\u001b[38;5;241m.\u001b[39mCOMPLETED\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:239\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[1;32m--> 239\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_trial(trial\u001b[38;5;241m.\u001b[39mtrial_id)\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mexists(\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mobjective\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m    242\u001b[0m     ):\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;66;03m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;66;03m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;66;03m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[0;32m    246\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe use case of calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    254\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    255\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:314\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(model_checkpoint)\n\u001b[0;32m    313\u001b[0m     copied_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m callbacks\n\u001b[1;32m--> 314\u001b[0m     obj_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_and_fit_model(trial, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcopied_kwargs)\n\u001b[0;32m    316\u001b[0m     histories\u001b[38;5;241m.\u001b[39mappend(obj_value)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:233\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m hp \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mhyperparameters\n\u001b[0;32m    232\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_build(hp)\n\u001b[1;32m--> 233\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhypermodel\u001b[38;5;241m.\u001b[39mfit(hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;66;03m# Save the build config for model loading later.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmulti_backend():\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py:149\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[1;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train the model.\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    370\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 371\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    372\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    217\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    218\u001b[0m     ):\n\u001b[1;32m--> 219\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1691\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1703\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "\n",
    "# Initialize the Keras Tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model_with_tuning,\n",
    "    objective='val_loss',\n",
    "    max_trials=5,  # Number of trials for hyperparameter search\n",
    "    executions_per_trial=1,\n",
    "    directory='tuner_dir',\n",
    "    project_name='image_captioning_tuning'\n",
    ")\n",
    "\n",
    "# Start the hyperparameter search\n",
    "tuner.search([train_img, train_seq], train_out, \n",
    "             validation_data=([val_img, val_seq], val_out),\n",
    "             epochs=10,  # Number of epochs to run for each trial\n",
    "             batch_size=64)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "print(\"Best hyperparameters:\", best_hp.values)\n",
    "\n",
    "# Build and train the model with the best hyperparameters\n",
    "model = build_model_with_tuning(best_hp)\n",
    "history = model.fit([train_img, train_seq], train_out, \n",
    "                    validation_data=([val_img, val_seq], val_out), \n",
    "                    epochs=20, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Best Model Saved by ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_7       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_6       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_3         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,175,232</span> │ input_layer_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_14        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ not_equal_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│                     │                   │            │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8497</span>)      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,183,729</span> │ dense_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_7       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_6       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_3         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │  \u001b[38;5;34m2,175,232\u001b[0m │ input_layer_7[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ input_layer_6[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ embedding_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_14        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ input_layer_7[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m524,544\u001b[0m │ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m525,312\u001b[0m │ dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ not_equal_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│                     │                   │            │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m65,792\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8497\u001b[0m)      │  \u001b[38;5;34m2,183,729\u001b[0m │ dense_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,474,609</span> (20.88 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,474,609\u001b[0m (20.88 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,474,609</span> (20.88 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,474,609\u001b[0m (20.88 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Layer, Input, Embedding, LSTM, Dense, Dropout, add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Function to build the CNN-LSTM model\n",
    "def build_model(vocab_size, max_length, feature_size):\n",
    "    # Image feature input\n",
    "    inputs1 = Input(shape=(feature_size,))\n",
    "    fe1 = Dropout(0.5)(inputs1)  # Apply dropout for regularization\n",
    "    fe2 = Dense(256, activation='relu')(fe1)  # Dense layer for image features\n",
    "\n",
    "    # Caption input\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)  # Embedding layer for captions\n",
    "    se2 = Dropout(0.5)(se1)  # Dropout layer for regularization\n",
    "    se3 = LSTM(256)(se2)  # LSTM layer for caption generation\n",
    "\n",
    "    # Merge image features and caption features\n",
    "    merged = add([fe2, se3])  # Merging image features and LSTM output\n",
    "    dense1 = Dense(256, activation='relu')(merged)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(dense1)  # Final softmax layer for output\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "vocab_size = 8497  # Update with the actual vocab size\n",
    "max_length = 40    # Update with the actual max length\n",
    "feature_size = 2048  # Update with the actual feature size from InceptionV3\n",
    "\n",
    "# Build the model\n",
    "model = build_model(vocab_size, max_length, feature_size)\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m4189/4189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m354s\u001b[0m 84ms/step - accuracy: 0.2621 - loss: 4.4691 - val_accuracy: 0.3551 - val_loss: 3.5048\n",
      "Epoch 2/5\n",
      "\u001b[1m4189/4189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 80ms/step - accuracy: 0.3579 - loss: 3.2806 - val_accuracy: 0.3755 - val_loss: 3.3240\n",
      "Epoch 3/5\n",
      "\u001b[1m4189/4189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m339s\u001b[0m 81ms/step - accuracy: 0.3793 - loss: 2.9879 - val_accuracy: 0.3861 - val_loss: 3.2853\n",
      "Epoch 4/5\n",
      "\u001b[1m4189/4189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m344s\u001b[0m 82ms/step - accuracy: 0.3932 - loss: 2.8094 - val_accuracy: 0.3927 - val_loss: 3.2905\n",
      "Epoch 5/5\n",
      "\u001b[1m4189/4189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m351s\u001b[0m 84ms/step - accuracy: 0.4039 - loss: 2.6813 - val_accuracy: 0.3960 - val_loss: 3.3302\n",
      "Validation Loss: 3.330164909362793\n",
      "Validation Accuracy: 0.395956426858902\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Function to quickly perform a simplified 2-fold cross-validation on a smaller subset of data\n",
    "def quick_cross_validate_model(model, train_img, train_seq, train_out, n_splits=2):\n",
    "    # Split data into training and validation set (just a quick version)\n",
    "    X_train_img, X_val_img, X_train_seq, X_val_seq, X_train_out, X_val_out = train_test_split(\n",
    "        train_img, train_seq, train_out, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Train the model on the training data\n",
    "    history = model.fit(\n",
    "        [X_train_img, X_train_seq], X_train_out,\n",
    "        validation_data=([X_val_img, X_val_seq], X_val_out),\n",
    "        epochs=5,  # Reduced number of epochs to save time\n",
    "        batch_size=64,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    val_loss, val_acc = model.evaluate([X_val_img, X_val_seq], X_val_out, verbose=0)\n",
    "    print(f\"Validation Loss: {val_loss}\")\n",
    "    print(f\"Validation Accuracy: {val_acc}\")\n",
    "\n",
    "# Assuming 'model' is already built and 'train_img', 'train_seq', 'train_out' are available\n",
    "quick_cross_validate_model(model, train_img, train_seq, train_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2232/2232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 18ms/step - accuracy: 0.3888 - loss: 3.4877\n",
      "Test Loss: 3.5352697372436523, Test Accuracy: 0.3839573264122009\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate([test_img, test_seq], test_out, verbose=1)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Captions for Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-extracted features...\n",
      "Features saved.\n",
      "Generating caption for image: D:/Desktop/DL project/Mini project - dataset/Mini project - dataset/Images/335588286_f67ed8c9f9.jpg\n",
      "Generated Caption: a dog is running through the sand\n",
      "--------------------------------------------------\n",
      "Generating caption for image: D:/Desktop/DL project/Mini project - dataset/Mini project - dataset/Images/507758961_e63ca126cc.jpg\n",
      "Generated Caption: a little girl in a pink shirt is jumping on a trampoline\n",
      "--------------------------------------------------\n",
      "Generating caption for image: D:/Desktop/DL project/Mini project - dataset/Mini project - dataset/Images/2021602343_03023e1fd1.jpg\n",
      "Generated Caption: a man in a white shirt is playing a game of basketball players\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Function to generate captions for a list of test images\n",
    "def generate_captions_for_test_images(model, image_paths, tokenizer, max_length, feature_file=\"D:/Desktop/DL project/Mini project - dataset/outputs/custom_image_features.pkl\"):\n",
    "    features = extract_image_features(image_paths, feature_file)  # Extract features for test images\n",
    "    for image_path in image_paths:\n",
    "        print(f\"Generating caption for image: {image_path}\")\n",
    "        try:\n",
    "            caption = generate_caption(model, image_path, tokenizer, max_length, features, feature_file)\n",
    "            print(f\"Generated Caption: {caption}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating caption for {image_path}: {e}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# List of images to generate captions for\n",
    "image_paths = [\n",
    "    \"D:/Desktop/DL project/Mini project - dataset/Mini project - dataset/Images/335588286_f67ed8c9f9.jpg\",\n",
    "    \"D:/Desktop/DL project/Mini project - dataset/Mini project - dataset/Images/507758961_e63ca126cc.jpg\",\n",
    "    \"D:/Desktop/DL project/Mini project - dataset/Mini project - dataset/Images/2021602343_03023e1fd1.jpg\"\n",
    "]\n",
    "\n",
    "# Generate captions for the test images\n",
    "generate_captions_for_test_images(model, image_paths, tokenizer, max_length)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
